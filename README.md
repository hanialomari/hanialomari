## Hi, I'm Hani Alomari ðŸ‘‹

I build retrieval systems that understand **images, text, video, and sound** - not just literal matches.

I'm a **PhD researcher at Virginia Tech**, working on **vision-language models (VLMs)**, **RAG**, and **ranking/reranking**. My focus is **multi-prompt (multi-vector) embeddings**: many small, controllable "views" of meaning that make search richer, more interpretable, and less prone to collapse.

### What I work on
- **Reasoning in vision-language models (VLMs)**.
- **Cross-modal retrieval** across images, text, video, and audio.
- **Structured information extraction** from multimodal data.
- **Knowledge representation** for multimodal reasoning.
- Exploring **room acoustics (RIRs)** as spatial signals for learning geometry-aware representations

### Why it matters
Real-world queries are polysemous: idioms, metaphor, culture, and context often matter more than surface similarity. I design retrieval pipelines that surface the *right* connections, not only the nearest neighbor.

---

## Projects (quick view)
- **Multi-Prompt Embedding for Retrieval**
  - One input -> multiple focused embeddings to boost recall and reduce length/bias collapse.
- **RAG + Reranker for Multimodal Search**
  - Lightweight bi-encoder retrieval + VLM reader + cross-encoder reranker for better final ranking.
- **Diversity-Aware VLM Retrieval**
  - Retrieves multiple perspectives (literal/figurative/emotional/abstract/background) instead of forcing a single vector.

---

## Tech I use (most often)

### Languages
<p>
  <img src="https://img.shields.io/badge/Python-FFD43B?style=for-the-badge&logo=python&logoColor=blue" />
  <img src="https://img.shields.io/badge/Bash-121011?style=for-the-badge&logo=gnu-bash&logoColor=white" />
</p>

### ML / Data
<p>
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=PyTorch&logoColor=white" />
  <img src="https://img.shields.io/badge/HuggingFace-FFD21E?style=for-the-badge&logo=huggingface&logoColor=black" />
  <img src="https://img.shields.io/badge/scikit_learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white" />
  <img src="https://img.shields.io/badge/Pandas-2C2D72?style=for-the-badge&logo=pandas&logoColor=white" />
  <img src="https://img.shields.io/badge/Numpy-777BB4?style=for-the-badge&logo=numpy&logoColor=white" />
  <img src="https://img.shields.io/badge/Plotly-3F4F75?style=for-the-badge&logo=plotly&logoColor=white" />
</p>

### Systems / Tools
<p>
  <img src="https://img.shields.io/badge/FAISS-0468D7?style=for-the-badge&logo=meta&logoColor=white" />
  <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" />
  <img src="https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black" />
  <img src="https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white" />
  <img src="https://img.shields.io/badge/LaTeX-47A141?style=for-the-badge&logo=LaTeX&logoColor=white" />
</p>

---

## Open to collaborations
If you are working on **diversity-aware retrieval**, **interpretable VLMs**, or **multimodal reasoning benchmarks**, lets talk.

### How to reach me
- Website: https://hanialomari.github.io/
- Google Scholar: https://scholar.google.com/citations?user=Ft_qTcwAAAAJ&hl=en
- LinkedIn: https://www.linkedin.com/in/hanialomari/
- Email: mailto:hani@vt.edu

<!-- Optional: GitHub stats (replace USERNAME if needed)
![GitHub Stats](https://github-readme-stats.vercel.app/api?username=hanialomari&show_icons=true)
![Top Langs](https://github-readme-stats.vercel.app/api/top-langs/?username=hanialomari&layout=compact)
-->
